defaults:
  - dset: wsj0_2
  - hydra/job_logging: colorlog
  - hydra/hydra_logging: colorlog

# Dataset related
sample_rate: 8000
segment: 4
stride: 1    # in seconds, how much to stride between training examples
pad: true   # if training sample is too short, pad it
cv_maxlen: 8
validfull: 1   # use entire samples at valid

# Logging and printing, and does not impact training
# 每个epoch打印的次数
num_prints: 60
device: cuda
num_workers: 14
verbose: 0
show: 0   # just show the model and its size and exit

# Checkpointing, by default automatically load last checkpoint
checkpoint: true
continue_from: '' # Only pass the name of the exp, like `exp_dset=wham`
                  # this arg is ignored for the naming of the exp!
continue_best: false
restart: False # Ignore existing checkpoints
checkpoint_file: checkpoint_stft.th
history_file: history_stft.json
# separation demos
samples_dir: samples_stft

# Other stuff
seed: 112
dummy:  # use this if you want twice the same exp, with a name

# Evaluation stuff
pesq: False # compute pesq?
eval_every: 1
keep_last: 0

# Optimization related
optim: adam  # can be either SGD or adam
lr: 8e-4
beta2: 0.999
# Gong Chao's new try
stft_loss: True
stft_sc_factor: .5
stft_mag_factor: .5
L2_factor: .5

epochs: 10
batch_size: 8
max_norm: 5
# learning rate scheduling
lr_sched: step # can be either step or plateau
step: 
  step_size: 4
  gamma: 0.2
plateau:  #早停
  factor: 0.5
  patience: 5

# Models
model: swave # either demucs or dwave
swave:
  N: 64 # tensor size
  L: 8  # 1D conv kernel size
  H: 64 # hidden size of LSTM
  R: 4  # number of repeats of the MULCAT block
  C: 2
  input_normalize: False

# Experiment launching, distributed
ddp: false
ddp_backend: nccl
rendezvous_file: ./rendezvous

# Internal config, don't set manually
rank:
world_size:

# Hydra config
hydra:
  run:
    dir: ./outputs/stft_${hydra.job.override_dirname}
  job:
    config:
      # configuration for the ${hydra.job.override_dirname} runtime variable
      override_dirname:
        kv_sep: '='
        item_sep: ','
        # Remove all paths, as the / in them would mess up things
        # Remove params that would not impact the training itself
        # Remove all slurm and submit params.
        # This is ugly I know...
        exclude_keys: [
          'hydra.job_logging.handles.file.filename',
          'dset.train', 'dset.valid', 'dset.test', 'dset.mix_json', 'dset.mix_dir',
          'num_prints', 'continue_from',
          'device', 'num_workers', 'print_freq', 'restart', 'verbose',
          'log', 'ddp', 'ddp_backend', 'rendezvous_file', 'rank', 'world_size']
  job_logging:
    handlers:
      file:
        class: logging.FileHandler
        mode: w
        formatter: colorlog
        filename: trainer.log
      console:
        class: logging.StreamHandler
        formatter: colorlog
        stream: ext://sys.stderr

  hydra_logging:
    handlers:
      console:
        class: logging.StreamHandler
        formatter: colorlog
        stream: ext://sys.stderr
